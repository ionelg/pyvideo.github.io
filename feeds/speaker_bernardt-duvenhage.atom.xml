<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="http://pyvideo.org/" rel="alternate"></link><link href="http://pyvideo.org/feeds/speaker_bernardt-duvenhage.atom.xml" rel="self"></link><id>http://pyvideo.org/</id><updated>2017-10-06T00:00:00+00:00</updated><entry><title>Semantic Concept Embedding for a natural language FAQ system</title><link href="http://pyvideo.org/pycon-za-2017/semantic-concept-embedding-for-a-natural-language-faq-system.html" rel="alternate"></link><published>2017-10-06T00:00:00+00:00</published><updated>2017-10-06T00:00:00+00:00</updated><author><name>Bernardt Duvenhage</name></author><id>tag:pyvideo.org,2017-10-06:pycon-za-2017/semantic-concept-embedding-for-a-natural-language-faq-system.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;For a number of months now work has been proceeding in order to bring
perfection to the crudely conceived idea of a super-positioning of
word vectors that would not only capture the tenor of a sentence in a
vector of similar dimension, but that is based on the high
dimensional manifold hypothesis to optimally retain the various
semantic concepts. Such a super-positioning of word vectors is called
the semantic concept embedding.&lt;/p&gt;
&lt;p&gt;Now basically the only new principle involved is that instead of using
the mean of the word vectors of a sentence one rather retains the
dominant semantic concepts over all of the words. A modification
informed by the aforementioned manifold hypothesis.&lt;/p&gt;
&lt;p&gt;The original implementation retains the absolute maximum value over each
of the dimensions of an embedding such as the GloVe embedding developed
at Stanford University. The use of these semantic concept vectors then
allows effective matching of users' questions to an online FAQ system
which in turn allows a natural language adaptation of said system that
easily achieves an F-score of 0.922 on the Quora dataset given only
three examples of how any particular question may be asked.&lt;/p&gt;
&lt;p&gt;The semantic concept embedding has now reached a high level of
development. First, an analysis of the word embedding is applied to find
the prepotent semantic concepts. The associated direction vectors are
then used to transform the embeddings in just the right way to optimally
detangle the principal manifolds and further increase the performance of
the natural language FAQ system.&lt;/p&gt;
&lt;p&gt;This talk will give an overview of:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;The problem of semantic sentence embedding.&lt;/li&gt;
&lt;li&gt;How NLTK, numpy, and Python machine learning frameworks are used to solve the problem.&lt;/li&gt;
&lt;li&gt;How semantic concept embedding is used for natural language FAQ systems in chatbots, etc.&lt;/li&gt;
&lt;/ul&gt;
</summary></entry></feed>