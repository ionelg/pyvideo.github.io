<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org</title><link>http://pyvideo.org/</link><description></description><lastBuildDate>Thu, 10 May 2018 00:00:00 +0000</lastBuildDate><item><title>Docker for Data Science</title><link>http://pyvideo.org/pycon-us-2018/docker-for-data-science.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Jupyter notebooks simplify the process of developing and sharing Data Science projects across groups and organizations. However, when we want to deploy our work into production, we need to extract the model from the notebook and package it up with the required artifacts (data, dependencies, configurations, etc) to ensure it works in other environments. Containerization technologies such as Docker can be used to streamline this workflow.&lt;/p&gt;
&lt;p&gt;This hands-on tutorial presents Docker in the context of Reproducible Data Science - from idea to application deployment. You will get a thorough introduction to the world of containers; learn how to incorporate Docker into various Data Science projects; and walk through the process of building a Machine Learning model in Jupyter and deploying it as a containerized Flask REST API.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Aly Sivji</dc:creator><pubDate>Thu, 10 May 2018 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2018-05-10:pycon-us-2018/docker-for-data-science.html</guid><category>jupyter</category><category>docker</category><category>data science</category></item><item><title>Containerize all the things</title><link>http://pyvideo.org/caipyra-2016/containerize-all-the-things.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Palestra do Andrews Medina no Caipyra 2016:&lt;/p&gt;
&lt;p&gt;Containerize all the things&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="http://talks.godoc.org/github.com/andrewsmedina/containerize-all-the-things/sample.slide#1"&gt;http://talks.godoc.org/github.com/andrewsmedina/containerize-all-the-things/sample.slide#1&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrews Medina</dc:creator><pubDate>Sat, 25 Jun 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-06-25:caipyra-2016/containerize-all-the-things.html</guid><category>docker</category><category>devops</category></item><item><title>Sparking Pandas: an experiment</title><link>http://pyvideo.org/pycon-italia-2017/sparking-pandas-an-experiment.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Pandas is a good library to deal with tabular data. What if you need to
manage an amount of data that doesn’t fit into memory? What if you want
to “distribute” your computations among multiple machines?&lt;/p&gt;
&lt;p&gt;Starting from a real scenario, Apache Spark will be presented as the
main tool to read and process collected data. It will be shown how a
Pandas-like syntax will come in handy to run aggregations, filtering and
grouping using a Spark Dataframe.&lt;/p&gt;
&lt;p&gt;A previous knowledge of Docker and Docker Compose will be very useful
while knowing MongoDB (where data will be fetched from) is not
mandatory. Basics of functional programming will help to understand
Spark inner logic.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Francesco Bruni</dc:creator><pubDate>Fri, 07 Apr 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-04-07:pycon-italia-2017/sparking-pandas-an-experiment.html</guid><category>microservices</category><category>Jupyter</category><category>mongodb</category><category>data-visualization</category><category>data-analysis</category><category>spark</category><category>docker</category></item><item><title>Dockerizing the Python</title><link>http://pyvideo.org/pycon-se-2017/dockerizing-the-python.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The focus of this talk will be on building and running python programs in a docker container. A major chunk of the talk will also concentrate on the docker setup &amp;amp; it’s complete echo system. The purpose behind the talk is the demonstrate the usage of docker in everyday python development &amp;amp; deployment environment.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Ambreen Sheikh</dc:creator><pubDate>Wed, 06 Sep 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-09-06:pycon-se-2017/dockerizing-the-python.html</guid><category>docker</category></item><item><title>Extend Docker using Python</title><link>http://pyvideo.org/pycon-israel-2017/extend-docker-using-python.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Docker is the most popular platform to run Python applications within containers. Many companies are using this platform to either deploy micro-services or test the code changes before merging it to production. Docker has 3 extension points: Drivers, Plugins and user-facing API. I am going to focus on the latter (user-facing API) and by the end of the talk, you will learn Docker's REST API and know how to extend Docker capabilities using Python.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Boaz Shuster</dc:creator><pubDate>Tue, 13 Jun 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-06-13:pycon-israel-2017/extend-docker-using-python.html</guid><category>docker</category></item><item><title>Running jupyter notebook remotely in a docker swarm cluster</title><link>http://pyvideo.org/pydata-barcelona-2017/running-jupyter-notebook-remotely-in-a-docker-swarm-cluster.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The current version of Jupyter Notebook computes the document state at the browser side, this is a problem if you run long jobs in a remote notebook from a laptop. If you close the browser you lose all the output of the current running cell. I will explain how we solved this problem in our lab. This solution it also allows a &amp;quot;walkie-talkie&amp;quot; like real-time collaboration.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The solution is based on running a docker container with a browser and a VNC server. All the remote access to the notebooks is done using Apache Guacamole a clientless remote desktop gateway. Everything is running on a dynamic docker swarm cluster of 20 nodes. As a lateral effect, this solution it also allows a real-time collaboration between users in a way that multiple users can access at the same time the same desktop (but they have to fight for the mouse and the keyboard).&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="https://github.com/jordeu/pytalks/tree/master/20170520_pydata_jupyter_in_a_docker_cluster"&gt;https://github.com/jordeu/pytalks/tree/master/20170520_pydata_jupyter_in_a_docker_cluster&lt;/a&gt;&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jordi Deu-Pons</dc:creator><pubDate>Sat, 20 May 2017 15:00:00 +0200</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-05-20:pydata-barcelona-2017/running-jupyter-notebook-remotely-in-a-docker-swarm-cluster.html</guid><category>jupyter notebook</category><category>docker</category><category>swarm</category></item><item><title>DIY Serverless Platform with Python3 and Docker</title><link>http://pyvideo.org/pycon-jamaica-2016/diy-serverless-platform-with-python3-and-docker.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;A walkthrough on how I built my own serverless platform to run both ephemeral and long-lasting functions in Python on top of Docker, capable of handling REST and websocket connections. Will go over architecture, show code, and discuss pain points as well as next steps.  #pyconjamaica2016&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Joir-dan Gumbs</dc:creator><pubDate>Fri, 18 Nov 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-11-18:pycon-jamaica-2016/diy-serverless-platform-with-python3-and-docker.html</guid><category>Serverless</category><category>Docker</category></item><item><title>Dev Ops meets Data Science Taking models from prototype to production with Docker</title><link>http://pyvideo.org/pydata-dc-2016/dev-ops-meets-data-science-taking-models-from-prototype-to-production-with-docker.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;We present the evolution of a model to a production API that can scale to large e-commerce needs. On the journey we discuss metrics of success and how to use the Kubernetes cluster manager and associated tools for deploy. In addition to the use of these tools we highlight how to make use of the cluster management system for further testing and experimentation with your models.&lt;/p&gt;
&lt;p&gt;The chasm between data science and dev ops is often wide and impenetrable, but the two fields have more in common than meets the eye. Every data scientist will be able to lean in and help their career by investing in a basic understanding the basic principles of dev ops. In this talk I present the notions of service level indicators, objectives, and agreements. I cover the rigorous monitoring and testing of services. Finally we demonstrate how to build a basic data science workflow and push to production level APIs with Docker and Kubernetes.&lt;/p&gt;
&lt;p&gt;Kubernetes is an opinionated container cluster manager with an easy to use, robust interface. It can be use on very small and very large clusters. Docker is a container system that allows one to build code in an isolated environment. Paired with a container manager such as Kubernetes we are able to manage millions of instances as needed for a production deployment. These tools are two of many different options but are considered among the best open source solutions available.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andy Terrel</dc:creator><pubDate>Sun, 09 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-09:pydata-dc-2016/dev-ops-meets-data-science-taking-models-from-prototype-to-production-with-docker.html</guid><category>Data</category><category>data science</category><category>docker</category><category>models</category><category>science</category></item><item><title>Setting up predictive analytics services with Palladium</title><link>http://pyvideo.org/pydata-berlin-2016/setting-up-predictive-analytics-services-with-palladium.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Description&lt;/p&gt;
&lt;p&gt;We will introduce Palladium, an open source framework for setting up predictive analytics services. It supports tasks like fitting, evaluating, storing, and distributing (predictive) models. Core ML processes are compatible with scikit-learn and a large number of scikit-learn’s features can be used. Besides the use of Palladium we will also show how to use it with Docker and Mesos / Marathon.&lt;/p&gt;
&lt;p&gt;Abstract&lt;/p&gt;
&lt;p&gt;In this talk, we will introduce Palladium, an open source framework for easily setting up predictive analytics services (&lt;a class="reference external" href="https://github.com/ottogroup/palladium"&gt;https://github.com/ottogroup/palladium&lt;/a&gt;). It supports tasks like fitting, evaluating, storing, distributing, and updating (predictive) models. Core machine learning processes are compatible with the open source machine learning library scikit-learn and thus, a large number of scikit-learn’s features can be used with Palladium. Although being implemented in Python, Palladium provides support for other languages and is shipped with examples how to integrate and expose R and Julia models. For an efficient deployment of services based on Palladium, a script to create Docker images automatically is provided. This talk will cover the use of Palladium including an example where a simple classification service is set up. We will also show how Docker and Mesos / Marathon can be used to deploy and scale Palladium-based services. Having basic knowledge about Machine Learning and/or scikit-learn would be an advantage when attending this talk.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andreas Lattner</dc:creator><pubDate>Tue, 07 Jun 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-06-07:pydata-berlin-2016/setting-up-predictive-analytics-services-with-palladium.html</guid><category>palladium</category><category>scikit-learn</category><category>docker</category><category>mesos</category><category>marathon</category><category>machine learning</category></item><item><title>Building Python apps with Docker</title><link>http://pyvideo.org/pytexas-2015/building-python-apps-with-docker.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;If you haven't heard of Docker yet, its a great tool that allows you
wrap up your app and everything it needs to run: code, runtime, and even
system libraries and guarantee that it will always run the same,
regardless of the environment (local machine, server, or even the
cloud). Whether you're deploying a web app, performing data analysis, or
creating local environments for your dev team or CI builds, Docker can
help.&lt;/p&gt;
&lt;p&gt;I'll give an introduction to Docker, an overview of some of the current
tools in the Docker ecosystem (Docker Machine and Docker Compose) and
demonstrate how to create, build, and deploy Python applications using
Docker.&lt;/p&gt;
&lt;p&gt;This talk is targeted towards web developers, data scientists, or really
anyone who develops using Python that would like to learn more about
Docker and how it can help their projects.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Mark Adams</dc:creator><pubDate>Fri, 09 Oct 2015 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2015-10-09:pytexas-2015/building-python-apps-with-docker.html</guid><category>Docker</category></item></channel></rss>