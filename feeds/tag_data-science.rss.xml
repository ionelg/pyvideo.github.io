<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org</title><link>http://pyvideo.org/</link><description></description><lastBuildDate>Thu, 10 May 2018 00:00:00 +0000</lastBuildDate><item><title>Docker for Data Science</title><link>http://pyvideo.org/pycon-us-2018/docker-for-data-science.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Jupyter notebooks simplify the process of developing and sharing Data Science projects across groups and organizations. However, when we want to deploy our work into production, we need to extract the model from the notebook and package it up with the required artifacts (data, dependencies, configurations, etc) to ensure it works in other environments. Containerization technologies such as Docker can be used to streamline this workflow.&lt;/p&gt;
&lt;p&gt;This hands-on tutorial presents Docker in the context of Reproducible Data Science - from idea to application deployment. You will get a thorough introduction to the world of containers; learn how to incorporate Docker into various Data Science projects; and walk through the process of building a Machine Learning model in Jupyter and deploying it as a containerized Flask REST API.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Aly Sivji</dc:creator><pubDate>Thu, 10 May 2018 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2018-05-10:pycon-us-2018/docker-for-data-science.html</guid><category>jupyter</category><category>docker</category><category>data science</category></item><item><title>An introduction to PyMC3</title><link>http://pyvideo.org/pycon-de-2017/an-introduction-to-pymc3.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Adrian Seyboldt&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I studied Mathematics and Bioinformatics in Bonn and Tübingen and I am a core developer of pymc3 since Feb 2017. Currently, I work for Quantopian on the development of Bayesian Methods for portfolio allocation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;PyMC3 allows you to build statistical models for a wide range of datasets, use those models to estimate underlying parameters, and compute the uncertainty about those parameters. In this talk I will try to give a gentle introduction to PyMC3, and help avoid common pitfalls for new users.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Some of the problems that are discussed in the context of the reproducibility crisis in science and statistics can be solved or alleviated by tools like PyMC3 or Stan. They allow users to build much more realistic models and get a full distribution of the possible values for parameters as output – instead of p-values that are often hard to interpret correctly. Thanks to Hamiltonian and Variational methods, they are more flexible and can be applied to larger problems than predecessors like JAGS and BUGS. However, these new methods also come with challenges. Writing good models isn't easy, and when inference algorithms cry out in pain, they need someone who listens to them. This talk uses some real-world applications to give an introduction to PyMC3, without requiring a lot of background in math, statistics or programming.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recorded at&lt;/strong&gt; PyCon.DE 2017 Karlsruhe: &lt;a class="reference external" href="https://de.pycon.org/"&gt;https://de.pycon.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video editing&lt;/strong&gt;: Sebastian Neubauer &amp;amp; Andrei Dan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Blender, Avidemux &amp;amp; Sonic Pi&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Adrian Seyboldt</dc:creator><pubDate>Wed, 25 Oct 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-10-25:pycon-de-2017/an-introduction-to-pymc3.html</guid><category>python</category><category>data-science</category><category>machine learning</category><category>analysis</category></item><item><title>Connecting PyData to other Big Data Landscapes using Arrow and Parquet</title><link>http://pyvideo.org/pycon-de-2017/connecting-pydata-to-other-big-data-landscapes-using-arrow-and-parquet.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Uwe L. Korn&lt;/strong&gt; (&amp;#64;xhochy)&lt;/p&gt;
&lt;p&gt;Uwe Korn is a Data Scientist at the Karlsruhe-based RetailTec company Blue Yonder. His expertise is on building architectures for machine learning services that are scalably usable for multiple customers aiming at high service availability as well as rapid prototyping of solutions to evaluate the feasibility of his design decisions. As part of his work to provide an efficient data interchange he became a core committer to the Apache Parquet and Apache Arrow projects.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;While Python itself hosts a wide range of machine learning and data tools, other ecosystems like the Hadoop world also provide beneficial tools that can be either connected via Apache Parquet files or in memory using Arrow. This talks shows recent developments that allow interoperation at speed.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Python has a vast amount of libraries and tools in its machine learning and data analysis ecosystem. Although it is clearly in competition with R here about the leadership, the world that has sprung out of the Hadoop ecosystem has established itself in the space of data engineering and also tries to provide tools for distributed machine learning. As these stacks run in different environments and are mostly developed by distinct groups of people, using them together has been a pain. While Apache Parquet has already proven itself as the gold standard for the exchange of DataFrames serialized to files, Apache Arrow recently got traction as the in-memory format for DataFrame exchange between different ecosystems.&lt;/p&gt;
&lt;p&gt;This talk will outline how Apache Parquet files can be used in Python and how they are structured to provide efficient DataFrame exchange. In addition to small code sample, this also includes an explanation of some interesting details of the file format. Additionally, the idea of Apache Arrow will be presented and taking Apache Spark (2.3) as an example to showcase how performance increases once DataFrames can be efficiently shared between Python and JVM processes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recorded at&lt;/strong&gt; PyCon.DE 2017 Karlsruhe: &lt;a class="reference external" href="https://de.pycon.org/"&gt;https://de.pycon.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video editing&lt;/strong&gt;: Sebastian Neubauer &amp;amp; Andrei Dan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Blender, Avidemux &amp;amp; Sonic Pi&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Uwe L. Korn</dc:creator><pubDate>Wed, 25 Oct 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-10-25:pycon-de-2017/connecting-pydata-to-other-big-data-landscapes-using-arrow-and-parquet.html</guid><category>data-science</category><category>hadoop</category><category>apache</category><category>arrow</category><category>parquet</category><category>pandas</category><category>pydata</category></item><item><title>Data Plumbing 101 - ETL Pipelines for Everyday Projects</title><link>http://pyvideo.org/pycon-de-2017/data-plumbing-101-etl-pipelines-for-everyday-projects.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Eberhard Hansis&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I have been writing code for more than two thirds of my life, mostly for scientific computing and data analysis. In the past couple of years, I have worked on a range of different data science projects, all using Python at their core. During this time, I repeatedly was tasked with making data usable by joining multiple sources into a clearly defined data model. Once you have done that, it is amazing how much real-life value you can generate with little more than a bit of statistics and visualization. The world is full of underused data, let’s change that!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;There is no data science without ETL! This presentation is about implementing maintainable data integration for your projects. We will have a first look a ‘Ozelot’, a library based on Luigi and SQLAlchemy that helps you get started with building ETL pipelines.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;ETL, the hard way&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;You are starting a new data science project, and you can’t wait to perform some machine learning magic. However, before getting to ML, you have to deal with its ugly sibling: ETL. Extracting, transforming and loading data (or, more generally, data integration) is an indispensable first step in almost any data project.&lt;/p&gt;
&lt;p&gt;In your project you will, most likely, have to extract data from various sources, clean it, link it and prepare it to your needs. You will start writing a first data integration script for some part of the process, then a second, then a third. At some point you will write an ugly ‘master’ script to keep your 17 import scripts in check and run them in just the right order. When you come back to the code later, you will have a hard time deciphering what you did and why, and what format the output data is in.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Pipelines to the rescue&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Implementing a proper data integration pipeline and a well-defined data model helps document your data flows and makes them traceable. More importantly, it simplifies the ETL development process, because it lets you easily re-run the whole process or parts of it. And you will have to modify and re-run your ETL, because your code changes, your output requirements change or the data changes.&lt;/p&gt;
&lt;p&gt;In this talk I propose a setup for building maintainable data integration pipelines for everyday projects. This setup is embodied by ‘Ozelot’, my brand-new Python library for ETL. It is based on Luigi for pipeline management and SQLAlchemy as ORM layer. Ozelot gives you core functionality to quickly start building your own solution, including an ORM base class, database connection management and Luigi task classes that play nice with the ORM. It comes with extensively documented examples that walk you through various aspects of data integration.&lt;/p&gt;
&lt;p&gt;The proposed setup works well for many small- to medium-sized projects -- projects, for which you previously might not have implemented a proper data integration pipeline. For big-data projects or those requiring live streaming data you probably want to consider alternative solutions.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Core principles of data integration&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Taking one step back, I propose the following core principles for maintainable data integration:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Any and all data manipulation happens in the pipeline, in a single code base.&lt;/li&gt;
&lt;li&gt;The pipeline represents all dependencies between data integration tasks.&lt;/li&gt;
&lt;li&gt;Each task has a method for rolling back its operations.&lt;/li&gt;
&lt;li&gt;Data is loaded into a single database, in a clearly defined, object-based data model that also encodes object relationships.&lt;/li&gt;
&lt;li&gt;The whole process is fully automatic and thereby reproducible and traceable.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I will discuss why I think that these principles are important, and how they are reflected in the proposed setup.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recorded at&lt;/strong&gt; PyCon.DE 2017 Karlsruhe: &lt;a class="reference external" href="https://de.pycon.org/"&gt;https://de.pycon.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video editing&lt;/strong&gt;: Sebastian Neubauer &amp;amp; Andrei Dan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Blender, Avidemux &amp;amp; Sonic Pi&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Eberhard Hansis</dc:creator><pubDate>Wed, 25 Oct 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-10-25:pycon-de-2017/data-plumbing-101-etl-pipelines-for-everyday-projects.html</guid><category>data-science</category><category>analytics</category><category>python</category></item><item><title>Data Science Project for Beginners</title><link>http://pyvideo.org/pycon-de-2017/data-science-project-for-beginners.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Natalie Speiser,Jens Beyer&lt;/strong&gt; (&amp;#64;natalie_lavrio)&lt;/p&gt;
&lt;p&gt;Natalie is a psychologist focused on statistics and machine learning for predictions. Jens is a pyhsicist turned consultant for IBM and d-fine and helped big companies with statistical models since 2009. Together, we founded LAVRIO.solutions and help our clients to make the most out of their data. In our recent data science projects, we faced specific hurdles which seem to be typical.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;AI and Machine Learning are taking over the world - but how do you actually start with understanding your data and predicting events? And what kind of &amp;quot;political&amp;quot; trouble could you run into? With examples from real projects, we try to give you a feeling for data science projects.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We will be talking about examples from projects we did as data science consultants. What should be organized before you go to a client (internal or external). How should the data look like. What are problems you have to face while working with the clients IT department. You get to see a rough draft of our code. It won't be a thorough manual, just our experiences and how we dealt with problems.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recorded at&lt;/strong&gt; PyCon.DE 2017 Karlsruhe: &lt;a class="reference external" href="https://de.pycon.org/"&gt;https://de.pycon.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video editing&lt;/strong&gt;: Sebastian Neubauer &amp;amp; Andrei Dan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Blender, Avidemux &amp;amp; Sonic Pi&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Natalie Speiser</dc:creator><pubDate>Wed, 25 Oct 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-10-25:pycon-de-2017/data-science-project-for-beginners.html</guid><category>business</category><category>data-science</category><category>use-case</category><category>python</category><category>machine learning</category></item><item><title>Deep Learning for Computer Vision</title><link>http://pyvideo.org/pycon-de-2017/deep-learning-for-computer-vision.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Alex Conway&lt;/strong&gt; (&amp;#64;alxcnwy)
Alex is the founder &amp;amp; CTO of NumberBoost, a startup that builds deep learning applications. He previously worked as a quant for a hedge fund and as a data scientist for an e-commerce company. He has an honours degree in actuarial science and a MSc in statistics. He is one of the organizers of the Cape Town Deep Learning meet-up and has built numerous computer vision systems that run at scale in production predicting labels for millions of images per day.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The state-of-the-art in image classification has skyrocketed thanks to the development of deep convolutional neural networks and increases in the amount of data and computing power available to train them. The top-5 error rate in the ImageNet competition to predict which of 1000 classes an image belongs to has plummeted from 28% error in 2010 to just 2.25% in 2017 (human level error is around 5%).&lt;/p&gt;
&lt;p&gt;In addition to being able to classify objects in images (including not hotdogs), deep learning can be used to automatically generate captions for images, convert photos into paintings, detect cancer in pathology slide images, and help self-driving cars ‘see’.&lt;/p&gt;
&lt;p&gt;The talk will give an overview of the cutting edge and some of the core mathematical concepts and will also include a short code-first tutorial to show how easy it is to get started using deep learning for computer vision in python…&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This talk is a crash course on convolutional neural networks and how to use them to solve 2 real-world applications at scale. The first is an image moderation system and the second is a visual similarity system where a user uploads an image of an item and the system returns visually similar items.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recorded at&lt;/strong&gt; PyCon.DE 2017 Karlsruhe: &lt;a class="reference external" href="https://de.pycon.org/"&gt;https://de.pycon.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video editing&lt;/strong&gt;: Sebastian Neubauer &amp;amp; Andrei Dan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Blender, Avidemux &amp;amp; Sonic Pi&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Alex Conway</dc:creator><pubDate>Wed, 25 Oct 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-10-25:pycon-de-2017/deep-learning-for-computer-vision.html</guid><category>business</category><category>data-science</category><category>use-case</category><category>deep learning</category><category>ai</category><category>machine learning</category></item><item><title>Effective Data Analysis with Pandas Indexes</title><link>http://pyvideo.org/pycon-de-2017/effective-data-analysis-with-pandas-indexes.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Pandas is the Swiss-Multipurpose Knife for Data Analysis in Python. In this talk we will look deeper into how to gain productivity utilizing Pandas powerful indexing and make advanced analytics a piece of cake. Pandas features multiple index types. This talk will give you a deep insight into the Pandas indexes and showcase the handiness of special Indexes as the TimeSeriesIndex.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recorded at&lt;/strong&gt; PyCon.DE 2017 Karlsruhe: &lt;a class="reference external" href="https://de.pycon.org/"&gt;https://de.pycon.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video editing&lt;/strong&gt;: Sebastian Neubauer &amp;amp; Andrei Dan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Blender, Avidemux &amp;amp; Sonic Pi&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Alexander Hendorf</dc:creator><pubDate>Wed, 25 Oct 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-10-25:pycon-de-2017/effective-data-analysis-with-pandas-indexes.html</guid><category>machine learning</category><category>analytics</category><category>data-science</category><category>business analytics</category></item><item><title>Flow is in the Air: Best Practices of Building Analytical Data Pipelines with Apache Airflow</title><link>http://pyvideo.org/pycon-de-2017/flow-is-in-the-air-best-practices-of-building-analytical-data-pipelines-with-apache-airflow.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Dominik Benz&lt;/strong&gt; (&amp;#64;john_maverick)&lt;/p&gt;
&lt;p&gt;Dominik Benz holds a PhD from the University of Kassel in the field of Data Mining on the Social Web. Since 2012 he is working as a Big Data Engineer at Inovex GmbH. In this time, he was involved in several projects concerned with establishing analytical data platforms in various companies. He is most experienced in tools around the Hadoop Ecosystem like Apache Hive and Spark, and has hands-on experience with productionizing analytical applications.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Apache Airflow is an Open-Source python project which facilitates an intuitive programmatic definition of analytical data pipelines. Based on 2+ years of productive experience, we summarize its core concepts, detail on lessons learned and set it in context with the Big Data Analytics Ecosystem.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Motivation &amp;amp; Outline&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Creating, orchestrating and running multiple data processing or analysis steps may cover a substantial portion of a Data Engineer and Data Scientist business. A widely adopted notion for this process is a &amp;quot;data pipeline&amp;quot; - which consists mainly of a set of &amp;quot;operators&amp;quot; which perform a particular action on data, with the possibility to specify dependencies among those. Real-Life examples may include:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Importing several files with different formats into a Hadoop platform, perform data cleansing, and training a machine learning model on the result&lt;/li&gt;
&lt;li&gt;perform feature extraction on a given dataset, apply an existing deep learning model to it, and write the results in the backend of a microservice&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Apache Airflow is an open-source Python project developed by AirBnB which facilitates the programmatic definition of such pipelines. Features which differentiate Airflow from similar projects like Apache Oozie, Luigi or Azkaban include (i) its pluggable architecture with several extension points (ii) the programmatic approach of &amp;quot;workflow is code&amp;quot; and (iii) its tight relationship with the the Python as well as the Big Data Analytics Ecosystem. Based on several years of productive usage, we briefly summarize the core concepts of Airflow, and detail in-depth on lessons learned and best practices from our experience. These include hints for getting efficient quickly with Airflow, approaches to structure workflows, integrating it in an enterprise landscape, writing plugins and extentions, and maintaining it in productive environment. We conclude with a comparison with other analytical workflow engines and summarize why we have chosen Airflow.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Questions answered by this talk&lt;/em&gt;&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;What are the core concepts of Apache Airflow?&lt;/li&gt;
&lt;li&gt;How can Airflow help me with moving data pipelines from analytics to production?&lt;/li&gt;
&lt;li&gt;Which concepts of Airflow make it more slim and more efficient compared to Apache Oozie?&lt;/li&gt;
&lt;li&gt;How can I specify dynamic dependencies at runtime between my analytical data processing steps?&lt;/li&gt;
&lt;li&gt;Which facilities does Airflow offer to enable automation and orchestration of analytical tasks?&lt;/li&gt;
&lt;li&gt;How can I extend the built-in facilities of Airflow by writing Python plugins?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;People who benefit most from this talk&lt;/em&gt;&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Data Scientists who are looking for a slim library to automate and control their data processing steps&lt;/li&gt;
&lt;li&gt;Data Engineers who want to save time debugging static workflow definitions (e.g. in XML)&lt;/li&gt;
&lt;li&gt;Project leaders interested in tools which lower the burden of moving from analytics to production&lt;/li&gt;
&lt;li&gt;Hadoop Cluster administrators eager to save cluster resources&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Recorded at&lt;/strong&gt; PyCon.DE 2017 Karlsruhe: &lt;a class="reference external" href="https://de.pycon.org/"&gt;https://de.pycon.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video editing&lt;/strong&gt;: Sebastian Neubauer &amp;amp; Andrei Dan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Blender, Avidemux &amp;amp; Sonic Pi&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Dominik Benz</dc:creator><pubDate>Wed, 25 Oct 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-10-25:pycon-de-2017/flow-is-in-the-air-best-practices-of-building-analytical-data-pipelines-with-apache-airflow.html</guid><category>workflow</category><category>data pipeline</category><category>data-science</category><category>analytics</category></item><item><title>Getting Scikit-Learn To Run On Top Of Pandas</title><link>http://pyvideo.org/pycon-de-2017/getting-scikit-learn-to-run-on-top-of-pandas.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Ami Tavory&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Ami is a data scientist at Facebook Research's Core Data Science group. He previously worked as a machine learning researcher in the fields of bioinformatics and algorithmic trading. In 2010 he received a Ph.D in Electrical Engineering from Tel Aviv University, in the field of financial information theory. His bachelor's and master's are from Tel Aviv University too.&lt;/p&gt;
&lt;p&gt;Ami uses Python and C++ for data analysis. He contributed to various open source projects, and is the author of a libstd C++ extension shipped with g++ (pb_ds: policy-based data structures).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Scikit-Learn is built directly over numpy, Python's numerical array library. Pandas adds to numpy metadata and higher-level munging capabilities. This talk describes how to intelligently auto-wrap Scikit-Learn for creating a version that can leverage pandas's added features.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Scikit-Learn is the de-facto standard Python library for general-purpose machine learning. It operates over NumPy, an efficient, but low-level, homogeneic array library. Pandas adds to NumPy metadata, heterogeneity, and higher-leve munging capabilities.&lt;/p&gt;
&lt;p&gt;In the field of visualization, newer generation libraries, e.g., Seaborn and Bokeh, are providing safer, more readable, and higher-level functionality, by operating over Pandas data structures. Some of these are implemented using Matplotlib, a lower-level NumPy-based plotting library.&lt;/p&gt;
&lt;p&gt;This talk describes a library for a Pandas-based version of sickit-learn. Here, too, giving a Pandas interface to a machine-learning library, provides code which is safer to use, more readable, and allows direct integration with Pandas's higher-level munging capabilities.&lt;/p&gt;
&lt;p&gt;Due to the large-scale, and evolving nature, of sicikit-learn's codebase, it is infeasible to manually wrap it. Except for a small number of intentional deviations from sickit-learn, the library wraps Scikit-Learn modules lazily through module and class introspection, and dynamic module loading.&lt;/p&gt;
&lt;p&gt;Following a short review of the relevant points of Pandas and Scikit-Learn, the talk is roughly divided into two aspects:     Scikit-Learn And Pandas User Perspective     Safety Advantages Of Pandas-Based Estimators     Using Metadata For Inter-Instance Aggregated Features And Cross-Validation     Using Metadata For Advanced Meta-Algorithms: Stacking, Nested Labeled And Stratified Cross-Valdiation     Python Develop Perspective     Unique Challenges Of Scikit-Learn Introspection And Decoration     Two Approaches For Wrapping Scikit-Learn Estimators     Lazy Dynamic Module Loading&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recorded at&lt;/strong&gt; PyCon.DE 2017 Karlsruhe: &lt;a class="reference external" href="https://de.pycon.org/"&gt;https://de.pycon.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video editing&lt;/strong&gt;: Sebastian Neubauer &amp;amp; Andrei Dan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Blender, Avidemux &amp;amp; Sonic Pi&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Ami Tavory</dc:creator><pubDate>Wed, 25 Oct 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-10-25:pycon-de-2017/getting-scikit-learn-to-run-on-top-of-pandas.html</guid><category>code-introspection</category><category>scikit-learn</category><category>pandas</category><category>data-science</category><category>python</category><category>machine learning</category></item><item><title>Large-scale machine learning pipelines using Luigi, PySpark and scikit-learn</title><link>http://pyvideo.org/pycon-de-2017/large-scale-machine-learning-pipelines-using-luigi-pyspark-and-scikit-learn.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Alexander Bauer&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Alexander Bauer holds a Ph.D. in computer science. He has around 10 years industry experience, currently leading a team of data scientists at Lidl, one of the largest global discount supermarket chains. He is a Kaggle Master and regular speaker at the Frankfurt Predictive Analytics Meetup. He believes in agile software development practices and promotes Python as a primary language for data science applications in production.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For prescriptive analytics applications, data science teams need to design, build and maintain complex machine learning pipelines. In this talk, we demonstrate how such pipelines can be implemented in a robust, scalable and extensible manner using Python, Luigi, PySpark and scikit-learn.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Data science teams working on real-world prescriptive analytics applications face the challenge to design, build and maintain considerably complex machine learning pipelines on a daily basis. Such pipelines include parsing data from multiple data sources, extracting relevant predictive features, executing training, validation, prediction steps and finally optimizing actions to meet desired business outcome so that they can be shared and visualized to business users. In this talk, we demonstrate how such pipelines can be implemented end-to-end in a robust, scalable and extensible manner using Python, Luigi, PySpark and scikit-learn. We will share our lessons learned from using this framework in a real-world demand forecasting use case.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recorded at&lt;/strong&gt; PyCon.DE 2017 Karlsruhe: &lt;a class="reference external" href="https://de.pycon.org/"&gt;https://de.pycon.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video editing&lt;/strong&gt;: Sebastian Neubauer &amp;amp; Andrei Dan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Blender, Avidemux &amp;amp; Sonic Pi&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Alexander Bauer</dc:creator><pubDate>Wed, 25 Oct 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-10-25:pycon-de-2017/large-scale-machine-learning-pipelines-using-luigi-pyspark-and-scikit-learn.html</guid><category>data-science</category><category>analytics</category><category>python</category><category>machine learning</category></item><item><title>Modern ETL-ing with Python and Airflow (and Spark)</title><link>http://pyvideo.org/pycon-de-2017/modern-etl-ing-with-python-and-airflow-and-spark.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Tamara Mendt&lt;/strong&gt; (&amp;#64;TamaraMendt)&lt;/p&gt;
&lt;p&gt;Tamara Mendt is a Data Engineer at HelloFresh, a meal kit delivery service headquartered in Berlin, and one of the top 3 tech startups to come out of Europe over the past 4 years. She devotes her time to building data pipelines and designing and maintaining the company's data infrastructure. Tamara has a computer engineering degree from her native country Venezuela, and an Erasmus Mundus Masters degree in IT for Business Intelligence. She wrote her Master thesis at the TU Berlin with the research group where Apache Flink was born. At HelloFresh she is continuing to work with distributed technologies such has Apache Hadoop, Apache Kafka and Apache Spark to cope with the scalability that the fast growing company requires for dealing with their data.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The challenge of data integration is real. The sheer amount of tools that exist to address this problem is proof that organizations struggle with it. This talk will discuss the inherent challenges of data integration, and show how it can be tackled using Python and Apache Airflow and Apache Spark.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The way organizations analyze their data has evolved very quickly since the beginning of the millennium. The development of Hadoop, and the explosion in the variety of data that companies are dealing with nowadays, has fostered the appearance of the concept of data lake, and the shift of traditional ETL (extract, transform, load), to ELT (extract, load, transform). Yet, the challenge of integrating data to obtain valuable insights still remains, and despite the hype and attention being focused on data, very few organizations have actually managed to become data driven. In this talk I will present insights into how we are currently building data pipelines using Python (as a replacement to high level ETL software), Apache Airflow as a scheduler to our coded transformations, and Apache Spark for achieve scalability. Though building data pipelines is not the only element required to become data driven, it is a crucial one, and I hope to encourage the audience to use these open source technologies in their own ETL-ing (or ELT-ing) efforts.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recorded at&lt;/strong&gt; PyCon.DE 2017 Karlsruhe: &lt;a class="reference external" href="https://de.pycon.org/"&gt;https://de.pycon.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video editing&lt;/strong&gt;: Sebastian Neubauer &amp;amp; Andrei Dan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Blender, Avidemux &amp;amp; Sonic Pi&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tamara Mendt</dc:creator><pubDate>Wed, 25 Oct 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-10-25:pycon-de-2017/modern-etl-ing-with-python-and-airflow-and-spark.html</guid><category>data</category><category>data-science</category><category>pipeline</category></item><item><title>Sport analysis with Python</title><link>http://pyvideo.org/pycon-de-2017/sport-analysis-with-python.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;ul class="simple"&gt;
&lt;li&gt;Give an example data of the IoT sport case for instance the information of football match of a team (the positions, velocities of each player with are recorded in every 20 millisecond).&lt;/li&gt;
&lt;li&gt;We use Python to analysis and processing data (calculate the match time, analyst the activities of each player such as time in the bench, time in the pitch, ... )&lt;/li&gt;
&lt;li&gt;We use Tableau to visualize data&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Recorded at&lt;/strong&gt; PyCon.DE 2017 Karlsruhe: &lt;a class="reference external" href="https://de.pycon.org/"&gt;https://de.pycon.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video editing&lt;/strong&gt;: Sebastian Neubauer &amp;amp; Andrei Dan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Blender, Avidemux &amp;amp; Sonic Pi&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Thuy Le</dc:creator><pubDate>Wed, 25 Oct 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-10-25:pycon-de-2017/sport-analysis-with-python.html</guid><category>devops</category><category>analytics</category><category>data-science</category><category>python</category></item><item><title>Synthetic Data for Machine Learning Applications</title><link>http://pyvideo.org/pycon-de-2017/synthetic-data-for-machine-learning-applications.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Dr. Hendrik Niemeyer&lt;/strong&gt; (&amp;#64;hniemeye)&lt;/p&gt;
&lt;p&gt;Data Scientist working on predictive analytics with data from pipeline inspection measurements.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In this talk I will show how we use real and synthetic data to create successful models for risk assessing pipeline anomalies. The main focus is the estimation of the difference in the statistical properties of real and generated data by machine learning methods.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;ROSEN provides predictive analytics for pipelines by detecting and risk assessing anomalies from data gathered by inline inspection measurement devices. Due to budget reasons (pipelines need to be dug up to get acess) ground truth data for machine learning applications in this field are usually scarce, imbalanced and not available for all existing configurations of measurement devices. This creates the need for synthetic data (using FEM simulations and unsupervised learning algorithms) in order to be able to create successful models.&lt;/p&gt;
&lt;p&gt;But a naive mixture of real-world and synthetic samples in a model does not necessarily yield to an increased predictive performance because of differences in the statistical distributions in feature space. I will show how we evaluate the use of synthetic data besides simple visual inspection. Manifold learning (e.g. TSNE) can be used to gain an insight whether real and generated data are inherently different.
Quantitative approaches like classifiers trained to discriminate between these types of data provide a non visual insight whether a &amp;quot;synthetic gap&amp;quot; in the feature distributions exists.&lt;/p&gt;
&lt;p&gt;If the synthetic data is useful for model building careful considerations have to be applied when constructing cross validation folds and test sets to prevent biased estimates of the model performance.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recorded at&lt;/strong&gt; PyCon.DE 2017 Karlsruhe: &lt;a class="reference external" href="https://de.pycon.org/"&gt;https://de.pycon.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video editing&lt;/strong&gt;: Sebastian Neubauer &amp;amp; Andrei Dan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Blender, Avidemux &amp;amp; Sonic Pi&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Hendrik Niemeyer</dc:creator><pubDate>Wed, 25 Oct 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-10-25:pycon-de-2017/synthetic-data-for-machine-learning-applications.html</guid><category>data-science</category><category>python</category><category>machine learning</category><category>ai</category></item><item><title>The eye of the Python, an eye tracking system. From zero to... what eye learned</title><link>http://pyvideo.org/pycon-de-2017/the-eye-of-the-python-an-eye-tracking-system-from-zero-to-what-eye-learned.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Samuel Muñoz Hidalgo&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I am Samuel (Samu for friends). With a curious mind I studied computer science, then focused in machine learning and IoT as a professional career. I haven't given up on my plan to take over the world; that's why my coworkers know a crazy idea is coming out, when I can't hide any longer a mischievous smile. I like to meet people and understand other points of view, and in return I like to show what I can do and teach what I have mastered. But, what drives me crazy is rollerskating with disco music.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Is it possible to predict the point in the screen where a person is looking at? Easy to say but hard to do. An eye tracking system is the perfect project to learn the difficulties of applied machine learning. From gathering training data to building the final software with an acceptable performance.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In this talk I will show how to build an eye tracking system from scratch.&lt;/p&gt;
&lt;p&gt;Once the approach (machine learning) and the tools (Python ecosystem) are set, the important tasks are:     Making users addicted to a game built with Pygame in order to generate data.     Unleashing the power of deep learning over GPU with Tensorflow to train an Artificial Neural Network.     Exploiting the training model in real time so as to control a computer with the eyes with PyAutoGUI.&lt;/p&gt;
&lt;p&gt;The path is full of pitfalls and every clear single step to the goal turns out to be a mountain of small but very important subtasks. Every iteration is a continuous struggle just to gain a bit of accuracy.&lt;/p&gt;
&lt;p&gt;Despite these efforts, at the very end we will see the difference between the theoretical and the real world. Our engineering skills will determine the usability of the new interface that allows us to move the mouse with our eyes; and we will learn that things don't need to be perfect, because humans are ...&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recorded at&lt;/strong&gt; PyCon.DE 2017 Karlsruhe: &lt;a class="reference external" href="https://de.pycon.org/"&gt;https://de.pycon.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video editing&lt;/strong&gt;: Sebastian Neubauer &amp;amp; Andrei Dan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Blender, Avidemux &amp;amp; Sonic Pi&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Samuel Muñoz Hidalgo</dc:creator><pubDate>Wed, 25 Oct 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-10-25:pycon-de-2017/the-eye-of-the-python-an-eye-tracking-system-from-zero-to-what-eye-learned.html</guid><category>data-science</category><category>use-case</category><category>python</category><category>ai</category><category>deep learning</category></item><item><title>The Python Ecosystem for Data Science: A Guided Tour</title><link>http://pyvideo.org/pycon-de-2017/the-python-ecosystem-for-data-science-a-guided-tour.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Christian Staudt&lt;/strong&gt; (&amp;#64;C_L_Staudt)&lt;/p&gt;
&lt;p&gt;I am an independent data scientist with a background in computer science, in-depth in algorithms, data analysis, high-performance computing and software engineering. My current interests include machine learning and data visualization.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Pythonistas have access to an extensive collection of tools for data analysis. The space of tools is best understood as an ecosystem: Libraries build upon each other, and a good library fills an ecological niche by doing certain jobs well. This is a guided tour of the Python data science ecosystem.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The Python Ecosystem for Data Science: A Guided Tour&lt;/p&gt;
&lt;p&gt;Python is on its way to becoming the lingua franca of data science, and Pythonistas have access to an impressive and extensive collection of tools for data analysis. Here, a data scientist needs to see the forest for the trees: The space of tools is best understood as an ecosystem, where libraries build upon each other, and where a good library fills an ecological niche by doing certain jobs well. This talk is a guided tour of the Python data science ecosystem. More than a list of libraries, it aims to provide some structure, classing tools by type of data, size of data, and type of analysis. In our tour, we visit a number of areas, including working with tabular data (numpy, pandas, dask, ...) and graph data (e.g. networkx), statistics (e.g. statsmodels), machine learning (scikit-learn, ...), data visualization (matplotlib, seaborn, bokeh, ...). Aspiring data scientists, and everyone else working with data, should find this useful for selecting the right tools for their next data-driven project.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recorded at&lt;/strong&gt; PyCon.DE 2017 Karlsruhe: &lt;a class="reference external" href="https://de.pycon.org/"&gt;https://de.pycon.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video editing&lt;/strong&gt;: Sebastian Neubauer &amp;amp; Andrei Dan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Blender, Avidemux &amp;amp; Sonic Pi&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Christian Staudt</dc:creator><pubDate>Wed, 25 Oct 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-10-25:pycon-de-2017/the-python-ecosystem-for-data-science-a-guided-tour.html</guid><category>use-case</category><category>business</category><category>ai</category><category>analytics</category><category>data-science</category><category>python</category><category>machine learning</category></item><item><title>Time series feature extraction with tsfresh - “get rich or die overfitting”</title><link>http://pyvideo.org/pycon-de-2017/time-series-feature-extraction-with-tsfresh-get-rich-or-die-overfitting.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Nils Braun&lt;/strong&gt; (&amp;#64;_nilsbraun)&lt;/p&gt;
&lt;p&gt;Currently I am doing my PhD in Particle Physics - which mainly involves development of software in a large collaboration. I love working with Python and C++ to process large amounts of data. Of course it needs to be processed as quickly as possible. I am working on the core reconstruction algorithms for our experiment, which are steered and controlled using Python. Apart from that, I was working as a Data Science Engineer for Blue Yonder, a leading machine learning company, where the idea for tsfresh was born. I am still heavily involved in the project. When I am not writing code, I am updating myself on the newest technical geek stuff (mostly cloud computing and deep learning) or play the guitar.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Have you ever thought about developing a time series model to predict stock prices? Or do you consider log time series from the operation of cloud resources as being more compelling? In this case you really should consider using the time series feature extraction package tsfresh for your project.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Trends such as the Internet of Things (IoT), Industry 4.0, and precision medicine are driven by the availability of cheap sensors and advancing connectivity, which among others increases the availability of temporally annotated data. The resulting time series are the basis for manifold machine learning applications. Examples are the classification of hard drives into risk classes concerning specific defect, the log analysis of server farms for detecting intruders, or regression tasks like the prediction of the remaining lifespan of machinery. Tsfresh also allows to easily setup a machine learning pipeline that predicts stock prices, which we will demonstrate live during the presentation ;). The problem of extracting and selecting relevant features for classification or regression is these domains is especially hard to solve, if each label or regression target is associated with several time series and meta-information simultaneously – which is a common pattern in industrial applications. This talk introduces a distributed and parallel feature extraction and selection algorithm – the recently published Python library tsfresh. The fully automated extraction and importance selection does not only allow to reach better machine learning classification scores, but in combination with the speed of the package, also allows to incorporate tsfresh into automated AI-pipelines.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recorded at&lt;/strong&gt; PyCon.DE 2017 Karlsruhe: &lt;a class="reference external" href="https://de.pycon.org/"&gt;https://de.pycon.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video editing&lt;/strong&gt;: Sebastian Neubauer &amp;amp; Andrei Dan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Blender, Avidemux &amp;amp; Sonic Pi&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Nils Braun</dc:creator><pubDate>Wed, 25 Oct 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-10-25:pycon-de-2017/time-series-feature-extraction-with-tsfresh-get-rich-or-die-overfitting.html</guid><category>pydata</category><category>time series</category><category>data-science</category><category>machine learning</category><category>python</category><category>ai</category></item><item><title>Turbodbc: Turbocharged database access for data scientists</title><link>http://pyvideo.org/pycon-de-2017/turbodbc-turbocharged-database-access-for-data-scientists.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Michael König&lt;/strong&gt; (&amp;#64;turbodbc)&lt;/p&gt;
&lt;p&gt;Michael is a senior software engineer at Blue Yonder GmbH. He holds a PhD in physics, practices test-driven development, and digs Clean Code in C++ and Python. In the last five years, he invested more money in table tennis gear than in smartphones.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Python's database API 2.0 is well suited for transactional database workflows, but not so much for column-heavy data science. This talk explains how the ODBC-based turbodbc database module extends this API with first-class, efficient support for familiar NumPy and Apache Arrow data structures.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This talk introduces the open source Python database module turbodbc. It uses standard ODBC drivers to connect with virtually any database and is a viable (and often faster) alternative to &amp;quot;native&amp;quot; Python drivers.&lt;/p&gt;
&lt;p&gt;Briefly recounting the painful story of how data scientists previously used our analytics database, I explain why turbodbc was created and what distinguishes it from other ODBC modules. Sketching the flow of data from databases via drivers and Python modules to consumable Python objects, I motivate a few extensions to the standard database API 2.0 that turbodbc has made. These extensions heavily use NumPy arrays and Apache Arrow tables to provide data scientists with both familiar and efficient binary data structures they can further work on. I conclude my talk with benchmark results for a few databases.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recorded at&lt;/strong&gt; PyCon.DE 2017 Karlsruhe: &lt;a class="reference external" href="https://de.pycon.org/"&gt;https://de.pycon.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video editing&lt;/strong&gt;: Sebastian Neubauer &amp;amp; Andrei Dan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Blender, Avidemux &amp;amp; Sonic Pi&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Michael König</dc:creator><pubDate>Wed, 25 Oct 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-10-25:pycon-de-2017/turbodbc-turbocharged-database-access-for-data-scientists.html</guid><category>numpy</category><category>database</category><category>python</category><category>data-science</category><category>analytics</category></item><item><title>How to use pandas the wrong way</title><link>http://pyvideo.org/pycon-italia-2017/how-to-use-pandas-the-wrong-way.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The &lt;strong&gt;pandas&lt;/strong&gt; library represents a very efficient and convenient tool
for data manipulation, but sometimes hides unexpected pitfalls which can
arise in various and sometimes unintelligible ways.&lt;/p&gt;
&lt;p&gt;By briefly referring to some aspects of the internals, I will review
specific situations in which a change of approach can, for instance,
make a difference in terms of performance.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;UPDATE (April 12, 2017) - SLIDES:&lt;/strong&gt; the talk had very few slides;
still, you can find those few, together with the notebooks I used live,
&lt;a class="reference external" href="https://pietrobattiston.it/python:pycon"&gt;here&lt;/a&gt;.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Pietro Battiston</dc:creator><pubDate>Sat, 08 Apr 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-04-08:pycon-italia-2017/how-to-use-pandas-the-wrong-way.html</guid><category>data-science</category><category>pandas</category><category>pydata</category></item><item><title>Data Science &amp; Data Visualization in Python. How to harness power of Python for social good?</title><link>http://pyvideo.org/pydata-berlin-2017/data-science-data-visualization-in-python-how-to-harness-power-of-python-for-social-good.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Python as an Open Data Science tool offers many libraries for data visualization and I will show you how to use and combine the best. I strongly believe that power of data is not only in the information &amp;amp; insight that data can provide us, Data is and can be really beautiful and can not only transform our perception but also the world that we all live in.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In my talk I will primarily focus on answering/offer the answer to these questions:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Why we need data science and why more and more people should be really interested in analyzing data and data visualization? (motivation)&lt;/li&gt;
&lt;li&gt;What is data science and how to start doing it in Python? (introduction of procedures, tools, most popular IDE-s for Python, etc.)&lt;/li&gt;
&lt;li&gt;What tools for data analysis and data visualization Python offers? (in each stage of analysis the best libraries will be shown for the specific purpose; as for data visualization we will focus particularly on Bokeh, Seaborn, Plotly and use of Jupyter Notebook and Plotly)&lt;/li&gt;
&lt;li&gt;How to 'unlock' the insight hidden in data through Python and how to use it to transform not only public administration or business, but ultimately the transformation of the whole society and economy towards the insight &amp;amp; knowledge based? (potential of data science)&lt;/li&gt;
&lt;li&gt;Open Data, Open Government Partnership, Open Public Administration &amp;amp; all the advantages of Open Data Science &amp;amp; Python. Data-Driven Approach. Everywhere. Now. (the end of talk +vision)&lt;/li&gt;
&lt;/ul&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Radovan Kavicky</dc:creator><pubDate>Fri, 30 Jun 2017 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-06-30:pydata-berlin-2017/data-science-data-visualization-in-python-how-to-harness-power-of-python-for-social-good.html</guid><category>python</category><category>data-science</category><category>data-visualization</category><category>analytics</category><category>PyData</category><category>PyDataBLN</category><category>PyDataBerlin</category><category>PyDataBA</category><category>PyDataBratislava</category><category>talk</category><category>Data</category><category>Bokeh</category><category>Social Good</category><category>datascience</category><category>jupyter</category><category>open science</category><category>open data science</category><category>DataVisualization</category><category>data-analysis</category><category>analysis</category><category>matplotlib</category><category>numpy</category><category>data wrangling</category><category>jupyter notebook</category><category>pandas</category><category>machine learning</category><category>deep learning</category><category>Open Data</category><category>Citizen Data Science</category></item><item><title>Marketing Data Science</title><link>http://pyvideo.org/pydata-barcelona-2017/marketing-data-science.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Marketing Data Science: how digital marketing needs data science to survive.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Digital Marketing is changing the way corporations and brands communicates with customers. The investments in Digital Marketing are skyrocketing around the world in a multi-billion industry.&lt;/p&gt;
&lt;p&gt;But consumers and customers (specially millenials) does not trust advertising. Different approaches are being made by corporations like Inbound Marketing Strategies. My presentation is about how Digital Marketing needs Data Science in order to better understand the customer needs and generate new niches of interest for companies. Companies investing in Digital Marketing should take a close look at Data Science Platforms like Python in order to better gather inisghts, create segments and personalice a customer experience.&lt;/p&gt;
&lt;p&gt;I will provide some short examples about how we are using python jupyter notebook environment in order to gain inisghts from customers using IBM Watson API, generating new segmentation and customer experiences.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Joaquin Pais</dc:creator><pubDate>Sun, 21 May 2017 15:45:00 +0200</pubDate><guid isPermaLink="false">tag:pyvideo.org,2017-05-21:pydata-barcelona-2017/marketing-data-science.html</guid><category>marketing</category><category>data science</category><category>pandas</category></item><item><title>Matplotlib Plot Tutorial: Histograms, Scatter Plots &amp; Legend</title><link>http://pyvideo.org/datacamp/Matplotlib-Plot-Tutorial-For-Beginners.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Matplotlib makes it easy to create meaningful and insightful plots. In this beginner video, you will learn how to build various types of data visualizations such as histograms, scatter plots and line plots. You will also see how to customize them to make them more visually appealing and interpretable.&lt;/p&gt;
&lt;p&gt;Want to do the corresponding exercises? Go to our &lt;cite&gt;Python For Data Science Tutorial &amp;lt;https://www.datacamp.com/courses/intro-to-python-for-data-science&amp;gt;&lt;/cite&gt; where you can do them for free.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Filip Schouwenaars</dc:creator><pubDate>Mon, 01 Feb 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-02-01:datacamp/Matplotlib-Plot-Tutorial-For-Beginners.html</guid><category>Matplotlib</category><category>data science</category><category>data visualization</category><category>tutorial</category><category>DataCamp</category></item><item><title>Dev Ops meets Data Science Taking models from prototype to production with Docker</title><link>http://pyvideo.org/pydata-dc-2016/dev-ops-meets-data-science-taking-models-from-prototype-to-production-with-docker.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;We present the evolution of a model to a production API that can scale to large e-commerce needs. On the journey we discuss metrics of success and how to use the Kubernetes cluster manager and associated tools for deploy. In addition to the use of these tools we highlight how to make use of the cluster management system for further testing and experimentation with your models.&lt;/p&gt;
&lt;p&gt;The chasm between data science and dev ops is often wide and impenetrable, but the two fields have more in common than meets the eye. Every data scientist will be able to lean in and help their career by investing in a basic understanding the basic principles of dev ops. In this talk I present the notions of service level indicators, objectives, and agreements. I cover the rigorous monitoring and testing of services. Finally we demonstrate how to build a basic data science workflow and push to production level APIs with Docker and Kubernetes.&lt;/p&gt;
&lt;p&gt;Kubernetes is an opinionated container cluster manager with an easy to use, robust interface. It can be use on very small and very large clusters. Docker is a container system that allows one to build code in an isolated environment. Paired with a container manager such as Kubernetes we are able to manage millions of instances as needed for a production deployment. These tools are two of many different options but are considered among the best open source solutions available.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andy Terrel</dc:creator><pubDate>Sun, 09 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-09:pydata-dc-2016/dev-ops-meets-data-science-taking-models-from-prototype-to-production-with-docker.html</guid><category>Data</category><category>data science</category><category>docker</category><category>models</category><category>science</category></item><item><title>You got your engineering in my Data Science: Addressing the reproducibility crisis</title><link>http://pyvideo.org/pydata-dc-2016/you-got-your-engineering-in-my-data-science-addressing-the-reproducibility-crisis.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="http://www.slideshare.net/jonbodner/you-got-your-engineering-in-my-data-science-addressing-the-reproducibility-crisis-with-software-engineering"&gt;http://www.slideshare.net/jonbodner/you-got-your-engineering-in-my-data-science-addressing-the-reproducibility-crisis-with-software-engineering&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Data science is the backbone of modern scientific discovery and industry. Unfortunately, multiple recent studies have been found to be unreliable and non-reproducible. Adopting techniques from software engineering might help mitigate some of these problems.&lt;/p&gt;
&lt;p&gt;Data science is the backbone of modern scientific discovery and industry. It makes sense of everything from cancer trials to package delivery logistics. But all is not well with data science. Over the past decade, multiple studies have been found to be unreliable and non-reproducible when other scientists tried to recreate their results. This is due to a variety of factors, including fraud, pressure to publish, improper data handling practices, and bugs in analytic tools.&lt;/p&gt;
&lt;p&gt;The problems faced by data science mirror problems that software engineering has been trying to solve. While there are no silver bullets to guarantee quality software, techniques have been developed over time that have improved quality and reliability. Some of these techniques, including open source, version control, automation, and fuzzing could be adapted to the data science domain to improve reliability and help address the reproducibility crisis.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jon Bodner</dc:creator><pubDate>Sun, 09 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-09:pydata-dc-2016/you-got-your-engineering-in-my-data-science-addressing-the-reproducibility-crisis.html</guid><category>Data</category><category>data science</category><category>engineering</category><category>reproducibility</category></item><item><title>Keynote: How Open Data Science Opens the World of Innovation</title><link>http://pyvideo.org/pydata-dc-2016/keynote-how-open-data-science-opens-the-world-of-innovation.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Innovation today appears to be instantaneous in large part due to open source technology. Open Data Science is no exception. Python, a pillar in the Open Data Science bedrock, is well positioned to harvest innovation in software and with Anaconda, it’s also well positioned to capitalize on the latest hardware innovations. Anaconda and Intel are blazing a path for the Python community to take advantage of cognitive computing, including machine learning and deep learning.&lt;/p&gt;
&lt;p&gt;In this keynote, Peter and Robert will talk about how Open Data Science––a connected ecosystem of data, analytics and compute––streamlines the path to high performance and innovation to achieve breakthrough results.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Robert Cohn</dc:creator><pubDate>Sat, 08 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-08:pydata-dc-2016/keynote-how-open-data-science-opens-the-world-of-innovation.html</guid><category>Data</category><category>data science</category><category>science</category></item><item><title>Scaling up to Big Data Devops for Data Science</title><link>http://pyvideo.org/pydata-dc-2016/scaling-up-to-big-data-devops-for-data-science.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Scaling up R/Python from a single machine to a cluster environment can be tricky. While there are many tools available that make the launching of a cluster relatively easy, they are not focused or optimized to the specific use case of analytics but mostly on operations. Come and learn about devops tips and tricks to optimize your transition into the big data world as a data scientist.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Marck Vaisman</dc:creator><pubDate>Sat, 08 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-08:pydata-dc-2016/scaling-up-to-big-data-devops-for-data-science.html</guid><category>big data</category><category>Data</category><category>data science</category><category>devops</category><category>scaling</category><category>science</category></item><item><title>When Worlds Collide: Productionalizing a Data Science Model</title><link>http://pyvideo.org/pydata-chicago-2016/when-worlds-collide-productionalizing-a-data-science-model.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Chicago 2016&lt;/p&gt;
&lt;p&gt;On our first data science project at Shiftgig, the data science and engineering teams had to build software that was production-ready while maintaining the flexibility of a data science sandbox. Although these seem like irreconcilable goals, they forced us to improve inter-team communication and ultimately helped create a great product. We’ll walk through our process and the lessons we learned.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tudor Radoaca</dc:creator><pubDate>Sun, 28 Aug 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-08-28:pydata-chicago-2016/when-worlds-collide-productionalizing-a-data-science-model.html</guid><category>Data</category><category>data science</category><category>model</category><category>science</category></item><item><title>Keynote: Using Data Science for Social Good: Examples, Opportunities, and Challenges</title><link>http://pyvideo.org/pydata-chicago-2016/keynote-using-data-science-for-social-good-examples-opportunities-and-challenges.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Chicago 2016&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Rayid Ghani</dc:creator><pubDate>Sat, 27 Aug 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-08-27:pydata-chicago-2016/keynote-using-data-science-for-social-good-examples-opportunities-and-challenges.html</guid><category>Data</category><category>data science</category><category>science</category></item><item><title>How do I apply a function to a pandas Series or DataFrame?</title><link>http://pyvideo.org/data-school/pandas-30-apply-function.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Have you ever struggled to figure out the differences between apply, map, and applymap? In this video, I'll explain when you should use each of these methods and demonstrate a few common use cases. Watch the end of the video for three important announcements!&lt;/p&gt;
&lt;p&gt;This is video 30 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kevin Markham</dc:creator><pubDate>Tue, 23 Aug 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-08-23:data-school/pandas-30-apply-function.html</guid><category>data science</category><category>data analysis</category><category>data wrangling</category><category>data processing</category><category>pandas</category><category>tutorial</category><category>Data School</category><category>NumPy</category></item><item><title>How do I create a pandas DataFrame from another object?</title><link>http://pyvideo.org/data-school/pandas-29-dummy-dataframe.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Have you ever needed to create a DataFrame of &amp;quot;dummy&amp;quot; data, but without reading from a file? In this video, I'll demonstrate how to create a DataFrame from a dictionary, a list, and a NumPy array. I'll also show you how to create a new Series and attach it to the DataFrame.&lt;/p&gt;
&lt;p&gt;This is video 29 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kevin Markham</dc:creator><pubDate>Tue, 16 Aug 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-08-16:data-school/pandas-29-dummy-dataframe.html</guid><category>data science</category><category>data analysis</category><category>data wrangling</category><category>data processing</category><category>pandas</category><category>tutorial</category><category>Data School</category><category>NumPy</category></item><item><title>How do I change display options in pandas?</title><link>http://pyvideo.org/data-school/pandas-28-customize-display.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Have you ever wanted to change the way your DataFrame is displayed? Perhaps you needed to see more rows or columns, or modify the formatting of numbers? In this video, I'll demonstrate how to change the settings for five common display options in pandas.&lt;/p&gt;
&lt;p&gt;This is video 28 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kevin Markham</dc:creator><pubDate>Tue, 09 Aug 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-08-09:data-school/pandas-28-customize-display.html</guid><category>data science</category><category>data analysis</category><category>data wrangling</category><category>data processing</category><category>pandas</category><category>tutorial</category><category>Data School</category></item><item><title>How do I avoid a SettingWithCopyWarning in pandas?</title><link>http://pyvideo.org/data-school/pandas-27-setting-with-copy-warning.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;If you've been using pandas for a while, you've likely encountered a SettingWithCopyWarning. The proper response is to modify your code appropriately, not to turn off the warning! In this video, I'll show you two common scenarios in which this warning arises, explain why it's occurring, and then demonstrate how to address it.&lt;/p&gt;
&lt;p&gt;This is video 27 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kevin Markham</dc:creator><pubDate>Tue, 02 Aug 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-08-02:data-school/pandas-27-setting-with-copy-warning.html</guid><category>data science</category><category>data analysis</category><category>data wrangling</category><category>data processing</category><category>pandas</category><category>tutorial</category><category>Data School</category><category>missing data</category></item><item><title>How do I find and remove duplicate rows in pandas?</title><link>http://pyvideo.org/data-school/pandas-26-duplicate-data.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;During the data cleaning process, you will often need to figure out whether you have duplicate data, and if so, how to deal with it. In this video, I'll demonstrate the two key methods for finding and removing duplicate rows, as well as how to modify their behavior to suit your specific needs.&lt;/p&gt;
&lt;p&gt;This is video 26 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kevin Markham</dc:creator><pubDate>Tue, 26 Jul 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-07-26:data-school/pandas-26-duplicate-data.html</guid><category>data science</category><category>data analysis</category><category>data wrangling</category><category>data processing</category><category>pandas</category><category>tutorial</category><category>Data School</category><category>duplicate data</category></item><item><title>How do I work with dates and times in pandas?</title><link>http://pyvideo.org/data-school/pandas-25-dates-and-times.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Let's say that you have dates and times in your DataFrame and you want to analyze your data by minute, month, or year. What should you do? In this video, I'll demonstrate how you can convert your data to &amp;quot;datetime&amp;quot; format, enabling you to access a ton of convenient attributes and perform datetime comparisons and mathematical operations.&lt;/p&gt;
&lt;p&gt;This is video 25 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kevin Markham</dc:creator><pubDate>Tue, 19 Jul 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-07-19:data-school/pandas-25-dates-and-times.html</guid><category>data science</category><category>data analysis</category><category>data wrangling</category><category>data processing</category><category>pandas</category><category>tutorial</category><category>Data School</category><category>data visualization</category></item><item><title>How do I create dummy variables in pandas?</title><link>http://pyvideo.org/data-school/pandas-24-dummy-variables.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;If you want to include a categorical feature in your machine learning model, one common solution is to create dummy variables. In this video, I'll demonstrate three different ways you can create dummy variables from your existing DataFrame columns. I'll also show you a trick for simplifying your code that was introduced in pandas 0.18.&lt;/p&gt;
&lt;p&gt;This is video 24 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kevin Markham</dc:creator><pubDate>Tue, 12 Jul 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-07-12:data-school/pandas-24-dummy-variables.html</guid><category>data science</category><category>data analysis</category><category>data wrangling</category><category>data processing</category><category>pandas</category><category>tutorial</category><category>Data School</category><category>machine learning</category></item><item><title>More of your pandas questions answered!</title><link>http://pyvideo.org/data-school/pandas-23-viewer-questions.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this video, I'm answering a few of the pandas questions I've received in the YouTube comments: Could you explain how to read the pandas documentation? What is the difference between ufo.isnull() and pd.isnull(ufo)? Why are DataFrame slices inclusive when using .loc, but exclusive when using .iloc? How do I randomly sample rows from a DataFrame?&lt;/p&gt;
&lt;p&gt;This is video 23 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kevin Markham</dc:creator><pubDate>Tue, 05 Jul 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-07-05:data-school/pandas-23-viewer-questions.html</guid><category>data science</category><category>data analysis</category><category>data wrangling</category><category>data processing</category><category>pandas</category><category>tutorial</category><category>Data School</category><category>reproducibility</category></item><item><title>How do I use pandas with scikit-learn to create Kaggle submissions?</title><link>http://pyvideo.org/data-school/pandas-22-prepare-for-machine-learning.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Have you been using scikit-learn for machine learning, and wondering whether pandas could help you to prepare your data and export your predictions? In this video, I'll demonstrate the simplest way to integrate pandas into your machine learning workflow, and will create a submission for Kaggle's Titanic competition in just a few lines of code!&lt;/p&gt;
&lt;p&gt;This is video 22 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kevin Markham</dc:creator><pubDate>Tue, 28 Jun 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-06-28:data-school/pandas-22-prepare-for-machine-learning.html</guid><category>data science</category><category>data analysis</category><category>data wrangling</category><category>data processing</category><category>pandas</category><category>tutorial</category><category>Data School</category><category>scikit-learn</category><category>machine learning</category></item><item><title>How do I make my pandas DataFrame smaller and faster?</title><link>http://pyvideo.org/data-school/pandas-21-reduce-dataframe-size.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Are you working with a large dataset in pandas, and wondering if you can reduce its memory footprint or improve its efficiency? In this video, I'll show you how to do exactly that in one line of code using the &amp;quot;category&amp;quot; data type, introduced in pandas 0.15. I'll explain how it works, and how to know when you shouldn't use it.&lt;/p&gt;
&lt;p&gt;This is video 21 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kevin Markham</dc:creator><pubDate>Tue, 21 Jun 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-06-21:data-school/pandas-21-reduce-dataframe-size.html</guid><category>data science</category><category>data analysis</category><category>data wrangling</category><category>data processing</category><category>pandas</category><category>tutorial</category><category>Data School</category></item><item><title>When should I use the "inplace" parameter in pandas?</title><link>http://pyvideo.org/data-school/pandas-20-inplace-parameter.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;We've used the &amp;quot;inplace&amp;quot; parameter many times during this video series, but what exactly does it do, and when should you use it? In this video, I'll explain how &amp;quot;inplace&amp;quot; affects methods such as &amp;quot;drop&amp;quot; and &amp;quot;dropna&amp;quot;, and why it is always False by default.&lt;/p&gt;
&lt;p&gt;This is video 20 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kevin Markham</dc:creator><pubDate>Tue, 14 Jun 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-06-14:data-school/pandas-20-inplace-parameter.html</guid><category>data science</category><category>data analysis</category><category>data wrangling</category><category>data processing</category><category>pandas</category><category>tutorial</category><category>Data School</category><category>missing data</category></item><item><title>How do I select multiple rows and columns from a pandas DataFrame?</title><link>http://pyvideo.org/data-school/pandas-19-select-dataframe-rows-and-columns.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Have you ever been confused about the &amp;quot;right&amp;quot; way to select rows and columns from a DataFrame? pandas gives you an incredible number of options for doing so, but in this video, I'll outline the current best practices for row and column selection using the loc, iloc, and ix methods.&lt;/p&gt;
&lt;p&gt;This is video 19 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kevin Markham</dc:creator><pubDate>Tue, 07 Jun 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-06-07:data-school/pandas-19-select-dataframe-rows-and-columns.html</guid><category>data science</category><category>data analysis</category><category>data wrangling</category><category>data processing</category><category>pandas</category><category>tutorial</category><category>Data School</category></item><item><title>What do I need to know about the pandas index? (Part 2)</title><link>http://pyvideo.org/data-school/pandas-18-index-part-2.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In part two of our discussion of the index, we'll switch our focus from the DataFrame index to the Series index. After discussing index-based selection and sorting, I'll demonstrate how automatic index alignment during mathematical operations and concatenation enables us to easily work with incomplete data in pandas.&lt;/p&gt;
&lt;p&gt;This is video 18 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kevin Markham</dc:creator><pubDate>Thu, 02 Jun 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-06-02:data-school/pandas-18-index-part-2.html</guid><category>data science</category><category>data analysis</category><category>data wrangling</category><category>data processing</category><category>pandas</category><category>tutorial</category><category>Data School</category><category>missing data</category></item><item><title>What do I need to know about the pandas index? (Part 1)</title><link>http://pyvideo.org/data-school/pandas-17-index-part-1.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The DataFrame index is core to the functionality of pandas, yet it's confusing to many users. In this video, I'll explain what the index is used for and why you might want to store your data in the index. I'll also demonstrate how to set and reset the index, and show how that affects the DataFrame's shape and contents.&lt;/p&gt;
&lt;p&gt;This is video 17 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kevin Markham</dc:creator><pubDate>Tue, 31 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-31:data-school/pandas-17-index-part-1.html</guid><category>data science</category><category>data analysis</category><category>data wrangling</category><category>data processing</category><category>pandas</category><category>tutorial</category><category>Data School</category></item><item><title>How do I handle missing values in pandas?</title><link>http://pyvideo.org/data-school/pandas-16-missing-values.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Most datasets contain &amp;quot;missing values&amp;quot;, meaning that the data is incomplete. Deciding how to handle missing values can be challenging! In this video, I'll cover all of the basics: how missing values are represented in pandas, how to locate them, and options for how to drop them or fill them in.&lt;/p&gt;
&lt;p&gt;This is video 16 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kevin Markham</dc:creator><pubDate>Thu, 26 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-26:data-school/pandas-16-missing-values.html</guid><category>data science</category><category>data analysis</category><category>data wrangling</category><category>data processing</category><category>pandas</category><category>tutorial</category><category>Data School</category><category>missing data</category></item><item><title>How do I explore a pandas Series?</title><link>http://pyvideo.org/data-school/pandas-15-explore-series.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;When you start working with a new dataset, how should you go about exploring it? In this video, I'll demonstrate some of the basic tools in pandas for exploring both numeric and non-numeric data. I'll also show you how to create simple visualizations in a single line of code!&lt;/p&gt;
&lt;p&gt;This is video 15 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kevin Markham</dc:creator><pubDate>Tue, 24 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-24:data-school/pandas-15-explore-series.html</guid><category>data science</category><category>data analysis</category><category>data wrangling</category><category>data processing</category><category>pandas</category><category>tutorial</category><category>Data School</category><category>data visualization</category></item><item><title>When should I use a "groupby" in pandas?</title><link>http://pyvideo.org/data-school/pandas-14-analyze-data-by-category.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The pandas &amp;quot;groupby&amp;quot; method allows you to split a DataFrame into groups, apply a function to each group independently, and then combine the results back together. This is called the &amp;quot;split-apply-combine&amp;quot; pattern, and is a powerful tool for analyzing data across different categories. In this video, I'll explain when you should use a groupby and then demonstrate its flexibility using four different examples.&lt;/p&gt;
&lt;p&gt;This is video 14 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kevin Markham</dc:creator><pubDate>Thu, 19 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-19:data-school/pandas-14-analyze-data-by-category.html</guid><category>data science</category><category>data analysis</category><category>data wrangling</category><category>data processing</category><category>pandas</category><category>tutorial</category><category>Data School</category><category>data visualization</category></item><item><title>How do I change the data type of a pandas Series?</title><link>http://pyvideo.org/data-school/pandas-13-change-data-type-of-series.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Have you ever tried to do math with a pandas Series that you thought was numeric, but it turned out that your numbers were stored as strings? In this video, I'll demonstrate two different ways to change the data type of a Series so that you can fix incorrect data types. I'll also show you the easiest way to convert a boolean Series to integers, which is useful for creating dummy/indicator variables for machine learning.&lt;/p&gt;
&lt;p&gt;This is video 13 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kevin Markham</dc:creator><pubDate>Tue, 17 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-17:data-school/pandas-13-change-data-type-of-series.html</guid><category>data science</category><category>data analysis</category><category>data wrangling</category><category>data processing</category><category>pandas</category><category>tutorial</category><category>Data School</category></item><item><title>How do I use string methods in pandas?</title><link>http://pyvideo.org/data-school/pandas-12-string-methods.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;pandas includes powerful string manipulation capabilities that you can easily apply to any Series of strings. In this video, I'll show you how to access string methods in pandas (along with a few examples), and then end with two bonus tips to help you maximize your efficiency.&lt;/p&gt;
&lt;p&gt;This is video 12 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kevin Markham</dc:creator><pubDate>Thu, 12 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-12:data-school/pandas-12-string-methods.html</guid><category>data science</category><category>data analysis</category><category>data wrangling</category><category>data processing</category><category>pandas</category><category>tutorial</category><category>Data School</category><category>regular expressions</category><category>string processing</category></item><item><title>How do I use the "axis" parameter in pandas?</title><link>http://pyvideo.org/data-school/pandas-11-dataframe-axis.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;When performing operations on a pandas DataFrame, such as dropping columns or calculating row means, it is often necessary to specify the &amp;quot;axis&amp;quot;. But what exactly is an axis? In this video, I'll help you to build a mental model for understanding the axis parameter so that you will know when and how to use it.&lt;/p&gt;
&lt;p&gt;This is video 11 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kevin Markham</dc:creator><pubDate>Tue, 10 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-10:data-school/pandas-11-dataframe-axis.html</guid><category>data science</category><category>data analysis</category><category>data wrangling</category><category>data processing</category><category>pandas</category><category>tutorial</category><category>Data School</category></item><item><title>Your pandas questions answered!</title><link>http://pyvideo.org/data-school/pandas-10-viewer-questions.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this video, I'm answering a few of the pandas questions I've received in the YouTube comments: When reading from a file, how do I read in only a subset of the columns or rows? How do I iterate through a Series or a DataFrame? How do I drop all non-numeric columns from a DataFrame? How do I know whether I should pass an argument as a string or a list?&lt;/p&gt;
&lt;p&gt;This is video 10 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kevin Markham</dc:creator><pubDate>Thu, 05 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-05:data-school/pandas-10-viewer-questions.html</guid><category>data science</category><category>data analysis</category><category>data wrangling</category><category>data processing</category><category>pandas</category><category>tutorial</category><category>Data School</category></item><item><title>How do I apply multiple filter criteria to a pandas DataFrame?</title><link>http://pyvideo.org/data-school/pandas-09-multiple-filter-criteria.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Let's say that you want to filter the rows of a DataFrame by multiple conditions. In this video, I'll demonstrate how to do this using two different logical operators. I'll also explain the special rules in pandas for combining filter criteria, and end with a trick for simplifying chained conditions!&lt;/p&gt;
&lt;p&gt;This is video 9 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kevin Markham</dc:creator><pubDate>Tue, 03 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-05-03:data-school/pandas-09-multiple-filter-criteria.html</guid><category>data science</category><category>data analysis</category><category>data wrangling</category><category>data processing</category><category>pandas</category><category>tutorial</category><category>Data School</category></item><item><title>How do I filter rows of a pandas DataFrame by column value?</title><link>http://pyvideo.org/data-school/pandas-08-filter-dataframe-rows.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Let's say that you only want to display the rows of a DataFrame which have a certain column value. How would you do it? pandas makes it easy, but the notation can be confusing and thus difficult to remember. In this video, I'll work up to the solution step-by-step using regular Python code so that you can truly understand the logic behind pandas filtering notation.&lt;/p&gt;
&lt;p&gt;This is video 8 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kevin Markham</dc:creator><pubDate>Thu, 28 Apr 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-04-28:data-school/pandas-08-filter-dataframe-rows.html</guid><category>data science</category><category>data analysis</category><category>data wrangling</category><category>data processing</category><category>pandas</category><category>tutorial</category><category>Data School</category></item><item><title>How do I sort a pandas DataFrame or a Series?</title><link>http://pyvideo.org/data-school/pandas-07-sort-dataframe-or-series.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;pandas allows you to sort a DataFrame by one of its columns (known as a &amp;quot;Series&amp;quot;), and also allows you to sort a Series alone. The sorting API changed in pandas version 0.17, so in this video, I'll demonstrate both the &amp;quot;old way&amp;quot; and the &amp;quot;new way&amp;quot; to sort. I'll also show you how to sort a DataFrame by multiple columns at once!&lt;/p&gt;
&lt;p&gt;This is video 7 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kevin Markham</dc:creator><pubDate>Tue, 26 Apr 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-04-26:data-school/pandas-07-sort-dataframe-or-series.html</guid><category>data science</category><category>data analysis</category><category>data wrangling</category><category>data processing</category><category>pandas</category><category>tutorial</category><category>Data School</category></item><item><title>How do I remove columns from a pandas DataFrame?</title><link>http://pyvideo.org/data-school/pandas-06-remove-dataframe-column.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;If you have DataFrame columns that you're never going to use, you may want to remove them entirely in order to focus on the columns that you do use. In this video, I'll show you how to remove columns (and rows), and will briefly explain the meaning of the &amp;quot;axis&amp;quot; and &amp;quot;inplace&amp;quot; parameters.&lt;/p&gt;
&lt;p&gt;This is video 6 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kevin Markham</dc:creator><pubDate>Thu, 21 Apr 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-04-21:data-school/pandas-06-remove-dataframe-column.html</guid><category>data science</category><category>data analysis</category><category>data wrangling</category><category>data processing</category><category>pandas</category><category>tutorial</category><category>Data School</category></item><item><title>How do I rename columns in a pandas DataFrame?</title><link>http://pyvideo.org/data-school/pandas-05-rename-dataframe-column.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;You will often want to rename the columns of a DataFrame so that their names are descriptive, easy to type, and don't contain any spaces. In this video, I'll demonstrate three different strategies for renaming columns so that you can choose the best strategy to fit your particular situation.&lt;/p&gt;
&lt;p&gt;This is video 5 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kevin Markham</dc:creator><pubDate>Tue, 19 Apr 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-04-19:data-school/pandas-05-rename-dataframe-column.html</guid><category>data science</category><category>data analysis</category><category>data wrangling</category><category>data processing</category><category>pandas</category><category>tutorial</category><category>Data School</category></item><item><title>Why do some pandas commands end with parentheses (and others don't)?</title><link>http://pyvideo.org/data-school/pandas-04-methods-and-attributes.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;To access most of the functionality in pandas, you have to call the methods and attributes of DataFrame and Series objects. In this video, I'll discuss some common methods and attributes, and show you how to tell the difference between them. (Hint: It's all about the parentheses!)&lt;/p&gt;
&lt;p&gt;This is video 4 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kevin Markham</dc:creator><pubDate>Thu, 14 Apr 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-04-14:data-school/pandas-04-methods-and-attributes.html</guid><category>data science</category><category>data analysis</category><category>data wrangling</category><category>data processing</category><category>pandas</category><category>tutorial</category><category>Data School</category></item><item><title>How do I select a pandas Series from a DataFrame?</title><link>http://pyvideo.org/data-school/pandas-03-select-series-from-dataframe.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;DataFrames and Series are the two main object types in pandas for data storage: a DataFrame is like a table, and each column of the table is called a Series. You will often select a Series in order to analyze or manipulate it. In this video, I'll show you how to select a Series using &amp;quot;bracket notation&amp;quot; and &amp;quot;dot notation&amp;quot;, and will discuss the limitations of dot notation. I'll also demonstrate how to create a new Series in a DataFrame.&lt;/p&gt;
&lt;p&gt;This is video 3 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kevin Markham</dc:creator><pubDate>Tue, 12 Apr 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-04-12:data-school/pandas-03-select-series-from-dataframe.html</guid><category>data science</category><category>data analysis</category><category>data wrangling</category><category>data processing</category><category>pandas</category><category>tutorial</category><category>Data School</category></item><item><title>What is pandas? (Introduction to the Q&amp;A series)</title><link>http://pyvideo.org/data-school/pandas-01-introduction.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;pandas is a full-featured Python library for data analysis, manipulation, and visualization. This video series is for anyone who wants to work with data in Python, regardless of whether you are brand new to pandas or have some experience.&lt;/p&gt;
&lt;p&gt;This is video 1 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kevin Markham</dc:creator><pubDate>Thu, 07 Apr 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-04-07:data-school/pandas-01-introduction.html</guid><category>data science</category><category>data analysis</category><category>data wrangling</category><category>data processing</category><category>pandas</category><category>tutorial</category><category>Data School</category></item><item><title>How do I read a tabular data file into pandas?</title><link>http://pyvideo.org/data-school/pandas-02-read-tabular-data-file.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;&amp;quot;Tabular data&amp;quot; is just data that has been formatted as a table, with rows and columns (like a spreadsheet). You can easily read a tabular data file into pandas, even directly from a URL! In this video, I'll walk you through how to do that, including how to modify some of the default arguments of the read_table function to solve common problems.&lt;/p&gt;
&lt;p&gt;This is video 2 of 30 in the series, &lt;a class="reference external" href="http://www.dataschool.io/easier-data-analysis-with-pandas/"&gt;Easier data analysis in Python with pandas&lt;/a&gt;. The notebook and datasets shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/pandas-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kevin Markham</dc:creator><pubDate>Thu, 07 Apr 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-04-07:data-school/pandas-02-read-tabular-data-file.html</guid><category>data science</category><category>data analysis</category><category>data wrangling</category><category>data processing</category><category>pandas</category><category>tutorial</category><category>Data School</category><category>csv</category></item><item><title>How to evaluate a classifier in scikit-learn</title><link>http://pyvideo.org/data-school/scikit-learn-09-evaluating-classification-models.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this video, you'll learn how to properly evaluate a classification model using a variety of common tools and metrics, as well as how to adjust the performance of a classifier to best match your business objectives. I'll start by demonstrating the weaknesses of classification accuracy as an evaluation metric. I'll then discuss the confusion matrix, the ROC curve and AUC, and metrics such as sensitivity, specificity, and precision. By the end of the video, you will have a solid foundation for intelligently evaluating your own classification model.&lt;/p&gt;
&lt;p&gt;This is the ninth video in the series, &lt;a class="reference external" href="http://www.dataschool.io/machine-learning-with-scikit-learn/"&gt;Introduction to machine learning with scikit-learn&lt;/a&gt;. The notebook and resources shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/scikit-learn-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kevin Markham</dc:creator><pubDate>Fri, 23 Oct 2015 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2015-10-23:data-school/scikit-learn-09-evaluating-classification-models.html</guid><category>machine learning</category><category>data science</category><category>scikit-learn</category><category>tutorial</category><category>Data School</category><category>model evaluation</category><category>classification</category><category>confusion matrix</category><category>ROC curve</category><category>AUC</category></item><item><title>How to find the best model parameters in scikit-learn</title><link>http://pyvideo.org/data-school/scikit-learn-08-parameter-tuning-with-grid-search.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this video, you'll learn how to efficiently search for the optimal tuning parameters (or &amp;quot;hyperparameters&amp;quot;) for your machine learning model in order to maximize its performance. I'll start by demonstrating an exhaustive &amp;quot;grid search&amp;quot; process using scikit-learn's GridSearchCV class, and then I'll compare it with RandomizedSearchCV, which can often achieve similar results in far less time.&lt;/p&gt;
&lt;p&gt;This is the eighth video in the series, &lt;a class="reference external" href="http://www.dataschool.io/machine-learning-with-scikit-learn/"&gt;Introduction to machine learning with scikit-learn&lt;/a&gt;. The notebook and resources shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/scikit-learn-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kevin Markham</dc:creator><pubDate>Wed, 15 Jul 2015 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2015-07-15:data-school/scikit-learn-08-parameter-tuning-with-grid-search.html</guid><category>machine learning</category><category>data science</category><category>scikit-learn</category><category>tutorial</category><category>Data School</category><category>cross-validation</category><category>model evaluation</category><category>parameter tuning</category><category>grid search</category></item><item><title>Selecting the best model in scikit-learn using cross-validation</title><link>http://pyvideo.org/data-school/scikit-learn-07-model-evaluation-with-cross-validation.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this video, we'll learn about K-fold cross-validation and how it can be used for selecting optimal tuning parameters, choosing between models, and selecting features. We'll compare cross-validation with the train/test split procedure, and we'll also discuss some variations of cross-validation that can result in more accurate estimates of model performance.&lt;/p&gt;
&lt;p&gt;This is the seventh video in the series, &lt;a class="reference external" href="http://www.dataschool.io/machine-learning-with-scikit-learn/"&gt;Introduction to machine learning with scikit-learn&lt;/a&gt;. The notebook and resources shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/scikit-learn-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kevin Markham</dc:creator><pubDate>Sun, 28 Jun 2015 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2015-06-28:data-school/scikit-learn-07-model-evaluation-with-cross-validation.html</guid><category>machine learning</category><category>data science</category><category>scikit-learn</category><category>tutorial</category><category>Data School</category><category>cross-validation</category><category>model evaluation</category><category>feature selection</category><category>parameter tuning</category></item><item><title>Data science in Python: pandas, seaborn, scikit-learn</title><link>http://pyvideo.org/data-school/scikit-learn-06-data-science-pipeline.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this video, we'll cover the data science pipeline from data ingestion (with pandas) to data visualization (with seaborn) to machine learning (with scikit-learn). We'll learn how to train and interpret a linear regression model, and then compare three possible evaluation metrics for regression problems. Finally, we'll apply the train/test split procedure to decide which features to include in our model.&lt;/p&gt;
&lt;p&gt;This is the sixth video in the series, &lt;a class="reference external" href="http://www.dataschool.io/machine-learning-with-scikit-learn/"&gt;Introduction to machine learning with scikit-learn&lt;/a&gt;. The notebook and resources shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/scikit-learn-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kevin Markham</dc:creator><pubDate>Thu, 28 May 2015 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2015-05-28:data-school/scikit-learn-06-data-science-pipeline.html</guid><category>machine learning</category><category>data science</category><category>scikit-learn</category><category>tutorial</category><category>Data School</category><category>pandas</category><category>seaborn</category><category>linear regression</category><category>model evaluation</category><category>feature selection</category><category>visualization</category></item><item><title>Comparing machine learning models in scikit-learn</title><link>http://pyvideo.org/data-school/scikit-learn-05-comparing-machine-learning-models.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;We've learned how to train different machine learning models and make predictions, but how do we actually choose which model is &amp;quot;best&amp;quot;? We'll cover the train/test split process for model evaluation, which allows you to avoid &amp;quot;overfitting&amp;quot; by estimating how well a model is likely to perform on new data. We'll use that same process to locate optimal tuning parameters for a KNN model, and then we'll re-train our model so that it's ready to make real predictions.&lt;/p&gt;
&lt;p&gt;This is the fifth video in the series, &lt;a class="reference external" href="http://www.dataschool.io/machine-learning-with-scikit-learn/"&gt;Introduction to machine learning with scikit-learn&lt;/a&gt;. The notebook and resources shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/scikit-learn-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kevin Markham</dc:creator><pubDate>Thu, 14 May 2015 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2015-05-14:data-school/scikit-learn-05-comparing-machine-learning-models.html</guid><category>machine learning</category><category>data science</category><category>scikit-learn</category><category>tutorial</category><category>Data School</category><category>model evaluation</category><category>overfitting</category></item><item><title>Training a machine learning model with scikit-learn</title><link>http://pyvideo.org/data-school/scikit-learn-04-training-a-machine-learning-model.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Now that we're familiar with the famous iris dataset, let's actually use a classification model in scikit-learn to predict the species of an iris! We'll learn how the K-nearest neighbors (KNN) model works, and then walk through the four steps for model training and prediction in scikit-learn. Finally, we'll see how easy it is to try out a different classification model, namely logistic regression.&lt;/p&gt;
&lt;p&gt;This is the fourth video in the series, &lt;a class="reference external" href="http://www.dataschool.io/machine-learning-with-scikit-learn/"&gt;Introduction to machine learning with scikit-learn&lt;/a&gt;. The notebook and resources shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/scikit-learn-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kevin Markham</dc:creator><pubDate>Wed, 29 Apr 2015 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2015-04-29:data-school/scikit-learn-04-training-a-machine-learning-model.html</guid><category>machine learning</category><category>data science</category><category>scikit-learn</category><category>tutorial</category><category>Data School</category><category>classification</category><category>KNN</category></item><item><title>Getting started in scikit-learn with the famous iris dataset</title><link>http://pyvideo.org/data-school/scikit-learn-03-getting-started-with-machine-learning.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Now that we've set up Python for machine learning, let's get started by loading an example dataset into scikit-learn! We'll explore the famous &amp;quot;iris&amp;quot; dataset, learn some important machine learning terminology, and discuss the four key requirements for working with data in scikit-learn.&lt;/p&gt;
&lt;p&gt;This is the third video in the series, &lt;a class="reference external" href="http://www.dataschool.io/machine-learning-with-scikit-learn/"&gt;Introduction to machine learning with scikit-learn&lt;/a&gt;. The notebook and resources shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/scikit-learn-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kevin Markham</dc:creator><pubDate>Tue, 21 Apr 2015 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2015-04-21:data-school/scikit-learn-03-getting-started-with-machine-learning.html</guid><category>machine learning</category><category>data science</category><category>scikit-learn</category><category>tutorial</category><category>Data School</category><category>NumPy</category></item><item><title>Setting up Python for machine learning: scikit-learn and IPython Notebook</title><link>http://pyvideo.org/data-school/scikit-learn-02-machine-learning-setup.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Want to get started with machine learning in Python? I'll discuss the pros and cons of the scikit-learn library, show how to install my preferred Python distribution, and demonstrate the basic functionality of the IPython Notebook. If you don't yet know any Python, I'll also provide four recommended resources for learning Python.&lt;/p&gt;
&lt;p&gt;This is the second video in the series, &lt;a class="reference external" href="http://www.dataschool.io/machine-learning-with-scikit-learn/"&gt;Introduction to machine learning with scikit-learn&lt;/a&gt;. The notebook and resources shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/scikit-learn-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kevin Markham</dc:creator><pubDate>Wed, 15 Apr 2015 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2015-04-15:data-school/scikit-learn-02-machine-learning-setup.html</guid><category>machine learning</category><category>data science</category><category>scikit-learn</category><category>tutorial</category><category>Data School</category><category>IPython notebook</category><category>Jupyter notebook</category></item><item><title>What is machine learning, and how does it work?</title><link>http://pyvideo.org/data-school/scikit-learn-01-what-is-machine-learning.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Have you heard of &amp;quot;machine learning&amp;quot;, and you're trying to figure out exactly what that means? I'll give you my definition, provide some examples of machine learning, and explain at a high level how machine learning &amp;quot;works&amp;quot;.&lt;/p&gt;
&lt;p&gt;This is the first video in the series, &lt;a class="reference external" href="http://www.dataschool.io/machine-learning-with-scikit-learn/"&gt;Introduction to machine learning with scikit-learn&lt;/a&gt;. The notebook and resources shown in the video are available on &lt;a class="reference external" href="https://github.com/justmarkham/scikit-learn-videos"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kevin Markham</dc:creator><pubDate>Tue, 07 Apr 2015 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2015-04-07:data-school/scikit-learn-01-what-is-machine-learning.html</guid><category>machine learning</category><category>data science</category><category>scikit-learn</category><category>tutorial</category><category>Data School</category><category>supervised learning</category><category>unsupervised learning</category></item><item><title>Constructing Models to Deal with Missing Data</title><link>http://pyvideo.org/scipy-2016/constructing-models-to-deal-with-missing-data-scipy-2016-deborah-hanus.html</link><description></description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Deborah Hanus</dc:creator><pubDate>Fri, 15 Jul 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-07-15:scipy-2016/constructing-models-to-deal-with-missing-data-scipy-2016-deborah-hanus.html</guid><category>SciPy 2016</category><category>data science</category></item><item><title>Give Your Data an Entrance Exam: Tools from Psychometrics for Data Quality Evaluation</title><link>http://pyvideo.org/scipy-2016/give-your-data-an-entrance-exam-tools-from-psychometrics-for-data-quality-evaluation-scipy-2016.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;We discuss a means by which item response theory (IRT), originally developed as a psychometric tool for assessing a
person's intellectual or academic ability given their
performance on a standardized test, can be used as a
data quality tool. Assuming that a dataset has an
underlying &amp;quot;ability&amp;quot; to train predictive models (where
the ability is specific to the type of dependent variable
being predicted), we build many models on top
of a variety of datasets to simultaneously assess the
best dataset for a given dependent variable as well
as which cases are the most &amp;quot;difficult&amp;quot; for a dataset
to predict correctly. The product of this work is an
understanding of both which predictions are the &amp;quot;hardest&amp;quot;
to get correct for any dataset, as well as which dataset
is expected to give the best predictions on a new
dependent variable.&lt;/p&gt;
&lt;p&gt;The first step in this study is to build a laboratory in which many related models can be trained and validated, reproducibly and
in a self-documenting way. By running many models that
look at related dependent variables, for example, a number
of variables meant to predict different aspects of political
behavior, we can characterize a baseline expected performance
for any new model similar to those already built.
We call this suite of related models a market basket, after the
terminology and methodology used by economists to summarize
the state of a market.&lt;/p&gt;
&lt;p&gt;Then, when we investigate new data sources or formats, we
have a well-defined process for determining whether the
new data makes the models better--we re-build our market
basket, and compare the results with the new data to the
results without it (performance, model build time,
data storage constraints) to assess the quality of our
data in a way that is driven by the models and data itself.&lt;/p&gt;
&lt;p&gt;An interesting question is how to assess whether a given
dataset or feature is &amp;quot;better&amp;quot; for a given basket of models. An interesting idea comes to us from the field of psychometrics, which uses a set of tools called item response theory to assess exams (such as the SAT and GRE) and use exams to rank students by intellectual or academic ability.&lt;/p&gt;
&lt;p&gt;Borrowing the terminology of IRT, we draw the analogy that a dataset is like a student (it has an inherent capability to accomplish certain tasks, like building good models), a single model prediction is a test question, and full set of test predictions is an exam. IRT parameterizes both the (unknown) student ability and the (also unknown) test question difficulty, and uses the EM algorithm to simultaneously solve for the parameters of both quantities at once. This allows a researcher to know both how &amp;quot;smart&amp;quot; a dataset is for solving a given basket of models, as well as rank-order &amp;quot;exam questions&amp;quot; (model predictions) by difficulty. The result is a single methodology with applications for both data quality and assessing the difficulty of making a given prediction (useful for e.g. outlier identification).&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Katie Malone</dc:creator><pubDate>Thu, 14 Jul 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-07-14:scipy-2016/give-your-data-an-entrance-exam-tools-from-psychometrics-for-data-quality-evaluation-scipy-2016.html</guid><category>data science</category></item></channel></rss>